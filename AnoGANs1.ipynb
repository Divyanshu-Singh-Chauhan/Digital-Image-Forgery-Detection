{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnoGANs",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GCPygJNR7uUU0MFiheaW16jspEqAdAfF",
      "authorship_tag": "ABX9TyPffrjY8VJo4QrB1Miew7Ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3f568261b7d24f69b40e5736fcf1a1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_753d8da106004ef58a958bf9060f1f2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_64d24d078aaa4af3a27e747687aae4e9",
              "IPY_MODEL_016b3e005ad1429ab33ab6cbd55ceea7"
            ]
          }
        },
        "753d8da106004ef58a958bf9060f1f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64d24d078aaa4af3a27e747687aae4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b38bd242706248589e5e7a7dc4158bb8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7491,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7491,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b20b3d9e08748009a5b7274880de0df"
          }
        },
        "016b3e005ad1429ab33ab6cbd55ceea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_387d05cbaea8442eba76c3e2372f1e3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7491/7491 [02:39&lt;00:00, 46.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a421c42f4107419a8df8236644949dcf"
          }
        },
        "b38bd242706248589e5e7a7dc4158bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b20b3d9e08748009a5b7274880de0df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "387d05cbaea8442eba76c3e2372f1e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a421c42f4107419a8df8236644949dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divyanshu-Singh-Chauhan/Image-Forgery-Detection/blob/DL/AnoGANs1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogq2r6i9FG3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4d9c006b-8255-4112-a1bc-f05543dddec1"
      },
      "source": [
        "\"\"\"\n",
        "Pipeline :\n",
        "\n",
        "1. Generate patches from the image.\n",
        "2. Train an Autoencoder on the full image rather than patches. Patches will be used while testing.\n",
        "3. Take the Encoder and extract a reduced dimension feature from it.\n",
        "4. Fit the one class SVM on the extracted feature layer .\n",
        "5. Predict on the image patches and produce the probability of mask.\n",
        "6. Generate the masked image back, with the mask probability generated.\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPipeline :\\n\\n1. Generate patches from the image.\\n2. Train an Autoencoder on the full image rather than patches. Patches will be used while testing.\\n3. Take the Encoder and extract a reduced dimension feature from it.\\n4. Fit the one class SVM on the extracted feature layer .\\n5. Predict on the image patches and produce the probability of mask.\\n6. Generate the masked image back, with the mask probability generated.\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2TOoDYn7pIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, UpSampling2D\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "from __future__ import print_function, division\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import argparse\n",
        "import keras.backend as K\n",
        "import pickle\n",
        "import sys \n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('agg')\n",
        "    import matplotlib.pyplot as plt\n",
        "except ImportError:\n",
        "    print(\"Failed to import matplotlib!\")\n",
        "    pass\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyW7KM0V0qHV",
        "colab_type": "text"
      },
      "source": [
        "# AUTOENCODER MODEL - ( DIS + GEN )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I24UJn0H3nn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Get and resize train images and masks ######\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "\n",
        "def get_data(path, train=False):\n",
        "    ids = os.listdir(path + \"/Tp\")\n",
        "    X = np.zeros((len(ids), img_height, img_width, 3), dtype=np.float32)\n",
        "    if train:\n",
        "        y = np.zeros((len(ids), img_height, img_width, 1), dtype=np.float32)\n",
        "    print('Getting and resizing images ... ')\n",
        "    for n, id_ in tqdm_notebook(enumerate(ids), total=len(ids)):\n",
        "        # Load images\n",
        "        try:\n",
        "          img = load_img(path + '/Tp/' + id_, grayscale=False, color_mode='rgb')\n",
        "          x_img = img_to_array(img)\n",
        "          x_img = resize(x_img, (img_height, img_width, 1), mode='constant', preserve_range=True)\n",
        "        except UnidentifiedImageError:\n",
        "          pass    \n",
        "        # Load masks\n",
        "        if train:\n",
        "            mask = img_to_array(load_img(path + '/masks/' + id_, grayscale=True))\n",
        "            mask = resize(mask, (img_height, img_width, 1), mode='constant', preserve_range=True)\n",
        "\n",
        "        # Save images\n",
        "        X[n, ..., 1] = x_img.squeeze() / 255\n",
        "        if train:\n",
        "            y[n] = mask / 255\n",
        "    print('Done!')\n",
        "    if train:\n",
        "        return X, y\n",
        "    else:\n",
        "        return X\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/CASIA2_COPY/CASIA2_ORIG/data/CASIA2')\n",
        "!unzip Tp.zip -d /content\n",
        "# os.chdir('/content')\n",
        "# !rm /content/Au/Thumbs.db  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWnEy9ssbw2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    img_rows = 128\n",
        "    img_cols = 128\n",
        "    channels = 3\n",
        "    img_shape = (img_rows, img_cols, channels)\n",
        "    encoded_shape = (8,8,128)\n",
        "\n",
        "\n",
        "    def build_encoder():\n",
        "        # Encoder\n",
        "        print(\"ENCODER--MODEL\")\n",
        "        encoder = Sequential()\n",
        "        encoder.add(Conv2D(16, kernel_size=6, strides=1, padding='same', input_shape = img_shape))\n",
        "        encoder.add(BatchNormalization())\n",
        "        encoder.add(Conv2D(16, kernel_size=5, strides=2, padding='same'))\n",
        "        encoder.add(BatchNormalization())\n",
        "        encoder.add(Conv2D(32, kernel_size=4, strides=2, padding='same'))\n",
        "        encoder.add(BatchNormalization())\n",
        "        encoder.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "        encoder.add(BatchNormalization())\n",
        "        encoder.add(Conv2D(128, kernel_size=2, strides=2, padding='same'))\n",
        "    \n",
        "        encoder.summary()\n",
        "\n",
        "        return encoder\n",
        "\n",
        "    def build_decoder():\n",
        "        # Decoder\n",
        "        print(\"DECODER--MODEL\")\n",
        "        decoder = Sequential()\n",
        "        decoder.add(Conv2DTranspose(64, kernel_size = 2, strides = 2, padding = 'same', input_shape = encoded_shape))\n",
        "        decoder.add(BatchNormalization())\n",
        "        decoder.add(Conv2DTranspose(32, kernel_size=3, strides=2, padding='same'))\n",
        "        decoder.add(BatchNormalization())\n",
        "        decoder.add(Conv2DTranspose(16, kernel_size=4, strides=2, padding='same'))\n",
        "        decoder.add(BatchNormalization())\n",
        "        decoder.add(Conv2DTranspose(16, kernel_size=5, strides=2, padding='same'))\n",
        "        decoder.add(BatchNormalization())\n",
        "        decoder.add(Conv2DTranspose(3, kernel_size=6, strides=1, padding='same'))\n",
        "        decoder.add(Activation(activation='tanh'))\n",
        "\n",
        "        decoder.summary()\n",
        "\n",
        "        return decoder\n",
        "\n",
        "    def build_discriminator():\n",
        "\n",
        "        print(\"DISCRiMINATOR--MODEL\")\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        print(\"1111\")\n",
        "        model.add(Conv2D(15, kernel_size = 5, strides = 1, input_shape = img_shape))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        print(\"2222\")\n",
        "        model.add(Conv2D(16, kernel_size = 2, strides = 2))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv2D(32, kernel_size = 4, strides = 1))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv2D(32, kernel_size = 2, strides = 2))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Conv2D(64, kernel_size = 3, strides = 1))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Conv2D(64, kernel_size = 2, strides = 2))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(128))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation(activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        return model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ianpDit1amga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f8e95c9-f1b6-40ae-b271-f86d7d583230"
      },
      "source": [
        "        # img_rows = 128\n",
        "        # img_cols = 128\n",
        "        # channels = 3\n",
        "        img_shape = (img_rows, img_cols, channels)\n",
        "        encoded_shape = (8,8,128)\n",
        "        history = {\"d_loss\": [], \"d_acc\": [], \"g_loss\": [], \"g_acc\": []}\n",
        "\n",
        "        optimizer = Adam(0.0005, 0.5)\n",
        "        # optimizer = RMSprop(lr=0.001, clipvalue=1.0, decay=3e-8)\n",
        "        # Build and compile the discriminator\n",
        "        discriminator = build_discriminator()\n",
        "        discriminator.trainable = False\n",
        "        discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build and compile the encoder / decoder\n",
        "        encoder = build_encoder()\n",
        "        decoder = build_decoder()\n",
        "\n",
        "        img = Input(shape=img_shape)\n",
        "        # The generator takes the image, encodes it and reconstructs it\n",
        "        # from the encoding\n",
        "        encoded_repr = encoder(img)\n",
        "        reconstructed_img = decoder(encoded_repr)\n",
        "        \n",
        "        print(\"The shape of recons-image is :\",reconstructed_img.shape)\n",
        "        autoencoder = Model(img, reconstructed_img)\n",
        "        autoencoder.compile(loss='mse', optimizer = optimizer)\n",
        "        # For the adversarial_autoencoder model we will only train the generator\n",
        "        #self.discriminator.trainable = False\n",
        "\n",
        "        print(\"ALERT!!!!!!! ERROR FROM HERE\")\n",
        "        # The discriminator determines validity of the encoding\n",
        "        validity = discriminator(reconstructed_img)      \n",
        "\n",
        "        # The adversarial_autoencoder model  (stacked generator and discriminator)\n",
        "        adversarial_autoencoder = Model(img, [reconstructed_img, validity])\n",
        "        adversarial_autoencoder.compile(loss=['mse', 'binary_crossentropy'], loss_weights=[0.99, 0.01], optimizer= optimizer, metrics=['accuracy'])\n",
        "        adversarial_autoencoder.summary()\n",
        "        print(adversarial_autoencoder.metrics_names)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DISCRiMINATOR--MODEL\n",
            "1111\n",
            "2222\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_23 (Conv2D)           (None, 124, 124, 15)      1140      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 124, 124, 15)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 124, 124, 15)      60        \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 62, 62, 16)        976       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 62, 62, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 62, 62, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 59, 59, 32)        8224      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 59, 59, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 59, 59, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 29, 29, 32)        4128      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 29, 29, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 27, 27, 64)        18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 27, 27, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 27, 27, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 13, 13, 64)        16448     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 13, 13, 64)        256       \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               1384576   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,435,009\n",
            "Trainable params: 1,434,563\n",
            "Non-trainable params: 446\n",
            "_________________________________________________________________\n",
            "ENCODER--MODEL\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_29 (Conv2D)           (None, 128, 128, 16)      1744      \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 128, 128, 16)      64        \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 64, 64, 16)        6416      \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 64, 64, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 32, 32, 32)        8224      \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 8, 8, 128)         32896     \n",
            "=================================================================\n",
            "Total params: 68,288\n",
            "Trainable params: 68,032\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "DECODER--MODEL\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_11 (Conv2DT (None, 16, 16, 64)        32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 32, 32, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 64, 64, 16)        8208      \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 64, 64, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DT (None, 128, 128, 16)      6416      \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 128, 128, 16)      64        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_15 (Conv2DT (None, 128, 128, 3)       1731      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 68,163\n",
            "Trainable params: 67,907\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "The shape of recons-image is : (None, None, None, 3)\n",
            "ALERT!!!!!!! ERROR FROM HERE\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "sequential_8 (Sequential)    (None, 8, 8, 128)         68288     \n",
            "_________________________________________________________________\n",
            "sequential_9 (Sequential)    (None, 128, 128, 3)       68163     \n",
            "_________________________________________________________________\n",
            "sequential_7 (Sequential)    (None, 1)                 1435009   \n",
            "=================================================================\n",
            "Total params: 1,571,460\n",
            "Trainable params: 135,939\n",
            "Non-trainable params: 1,435,521\n",
            "_________________________________________________________________\n",
            "['loss', 'sequential_9_loss', 'sequential_7_loss', 'sequential_9_accuracy', 'sequential_7_accuracy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZLVTWyueMcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def pretrain_ae(data, iterations, batch_size):\n",
        "        history = {'loss': []}\n",
        " \n",
        "        for it in range(iterations):\n",
        "            idx = np.random.randint(0, data.shape[0], batch_size)\n",
        "            imgs = data[idx]\n",
        "            train_loss = autoencoder.train_on_batch(imgs, imgs)\n",
        "            history['loss'].append(train_loss)\n",
        "\n",
        "            print(\"[Pretrain AE]---It {}/{} | AE loss: {:.4f}\".format(it, iterations, train_loss),flush=True)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.title(\"Pretrain AE\")\n",
        "        plt.xlabel(\"Iter\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        step = len(history['loss']) // 10 if len(history['loss']) > 1000 else 1\n",
        "        plt.plot(np.arange(len(history['loss'][::step])), history['loss'][::step])\n",
        "        plt.savefig(\"pretrain ae\")\n",
        "        \n",
        "        autoencoder.save_weights(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxY0SOaYhOm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def pretrain_discriminator(data, iterations, batch_size):\n",
        "        half_batch = batch_size // 2\n",
        "        fake = np.zeros((half_batch, 1))\n",
        "        valid = np.ones((half_batch, 1))\n",
        "        history = {'loss': [], 'acc': []}\n",
        "  \n",
        "        for it in range(iterations):\n",
        "            idx = np.random.randint(0, data.shape[0], half_batch)\n",
        "            imgs = data[idx]\n",
        "            generated_imgs = autoencoder.predict(imgs)\n",
        "            \n",
        "        \n",
        "            # Train the discriminator\n",
        "            d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            history['loss'].append(d_loss[0])\n",
        "            history['acc'].append(d_loss[1])\n",
        "            print(\"[Pretrain Discriminator]---it {}/{} | loss: {:.4f} | acc {:.2f}\".format(it, iterations, d_loss[0], d_loss[1]), flush=True)\n",
        "        \n",
        "        plt.figure()\n",
        "        plt.title(\"Pretrain Discriminator\")\n",
        "        plt.xlabel(\"Iter\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        step = len(history['loss']) // 10 if len(history['loss']) > 1000 else 1\n",
        "        plt.plot(np.arange(len(history['loss'][::step])), history['loss'][::step])\n",
        "        plt.savefig(\"pretrain discriminator\")\n",
        "\n",
        "        discriminator.save_weights(\"/content/drive/My Drive/weights/AnoGANs/discriminator.h5\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdfg-PnYeSQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_images(epoch,images):\n",
        "      rows, cols = 5, 5\n",
        "      if not os.path.isdir(\"/content/drive/My Drive/Colab Notebooks/AnoGANs/AE_images\"):\n",
        "        os.mkdir(\"/content/drive/My Drive/Colab Notebooks/AnoGANs/AE_images\")\n",
        "      imgs = autoencoder.predict(images)\n",
        "      print(\"The shape of img is : \", img.shape)\n",
        "        #RESCALE IMAGES\n",
        "      imgs = 0.5*imgs + 0.5\n",
        "\n",
        "      fig, axs = plt.subplots(rows,cols)\n",
        "      idx = 0\n",
        "      for i in range(rows):\n",
        "        for j in range(cols):\n",
        "          axs[i,j].imshow(imgs[idx],cmap = 'gray')\n",
        "          axs[i,j].axis('off')\n",
        "          idx += 1\n",
        "          fig.savefig(\"/content/drive/My Drive/Colab Notebooks/AnoGANs/AE_images/%d.png\" %epoch)\n",
        "      plt.close()        "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puf_N3xBfCcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    def train(iterations, pre_dis_iterations, pre_ae_iterations, batch_size=128, sample_interval=10, tolerance=20):\n",
        "\n",
        "        # Load the dataset\n",
        "        #X_train = get_data(train_path, train=False)\n",
        "        # X_train = np.load(\"data.npy\")\n",
        "        # mean = np.mean(X_train, axis=(0,1,2,3))\n",
        "        # std  = np.std(X_train, axis=(0,1,2,3))\n",
        "        # X_train = (X_train.astype(np.float32) - mean) / (std + 1e-7)\n",
        "        print(\"Start training on {} images\".format(X_train.shape[0]))\n",
        "\n",
        "        if os.path.isfile(\"/content/drive/My Drive/weights/AnoGANs/discriminator.h5\"):\n",
        "            discriminator.load_weights(\"/content/drive/My Drive/weights/AnoGANs/discriminator.h5\")\n",
        "            print(\"Loaded discriminator weights!\")\n",
        "        elif pre_dis_iterations > 0:\n",
        "            pretrain_discriminator(X_train, pre_dis_iterations, batch_size)\n",
        "            print(\"Discriminator Weights Initialised\")\n",
        "        \n",
        "        \n",
        "        if os.path.isfile(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\"):\n",
        "            autoencoder.load_weights(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\")\n",
        "            print(\"Loaded autoencoder weights!\")\n",
        "        elif pre_ae_iterations > 0:\n",
        "            pretrain_ae(X_train, pre_ae_iterations, batch_size)\n",
        "            print(\"Autoencoder Weights Initialised\")\n",
        "\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        half_batch = batch_size // 2\n",
        "        half_valid = np.ones((half_batch, 1))\n",
        "        half_fake = np.ones((half_batch, 1))\n",
        "        for it in range(iterations):\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            print(\"The epoch is : \", it)\n",
        "            imgs = X_train[np.random.randint(0, X_train.shape[0], half_batch)]\n",
        "          \n",
        "            generated_imgs = autoencoder.predict(imgs)\n",
        "            \n",
        "            \n",
        "            d_loss_real = discriminator.train_on_batch(imgs, half_valid)\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_imgs, half_fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            history['d_loss'].append(d_loss[0])\n",
        "            history['d_acc'].append(d_loss[1] * 100)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            \n",
        "            imgs = X_train[np.random.randint(0, X_train.shape[0], batch_size)]\n",
        "            g_loss = adversarial_autoencoder.train_on_batch(imgs, [imgs, valid])\n",
        "            history['g_loss'].append(g_loss[0])\n",
        "            history['g_acc'].append(g_loss[-1]*100)\n",
        "            print(\"[Training Adversarial AE]---epoch {}/{} | d_loss: {:.4f} | d_acc: {:.2f} | g_loss: {:.4f} | g_acc: {:.2f}\".format(it, iterations, d_loss[0], d_loss[1]*100, g_loss[0], g_loss[-1]*100),flush=True)\n",
        "            \n",
        "            \n",
        "            #If at save interval => save generated image samples\n",
        "            if it % sample_interval == 0:\n",
        "                # Select a random half batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size//2)\n",
        "                imgs = X_train[idx]\n",
        "                sample_images(it, imgs)\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXeb_dJQgbtl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229,
          "referenced_widgets": [
            "3f568261b7d24f69b40e5736fcf1a1b4",
            "753d8da106004ef58a958bf9060f1f2f",
            "64d24d078aaa4af3a27e747687aae4e9",
            "016b3e005ad1429ab33ab6cbd55ceea7",
            "b38bd242706248589e5e7a7dc4158bb8",
            "0b20b3d9e08748009a5b7274880de0df",
            "387d05cbaea8442eba76c3e2372f1e3d",
            "a421c42f4107419a8df8236644949dcf"
          ]
        },
        "outputId": "a41969e4-afb8-4739-aedc-106dbc147f6a"
      },
      "source": [
        "train_path = '/content'  \n",
        "tampered = get_data(train_path, train=False)\n",
        "#train(iterations=1000, pre_ae_iterations = 1000, pre_dis_iterations = 1000, batch_size = 128)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting and resizing images ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f568261b7d24f69b40e5736fcf1a1b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=7491.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 2. Skipping tag 41487\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 41988\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukKFklKixA70",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING THE ONE CLASS SVM AND SAVING THE PARAS OF THE SVM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN5SDoxExqAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### LOADING THE TRAINED WEIGHTS ####\n",
        "import os \n",
        "if os.path.isfile(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\"):\n",
        "  autoencoder.load_weights(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\")\n",
        "\n",
        "\n",
        "##### Extracting the Features from the dataset #####  \n",
        "def get_features(data):\n",
        "    preds = encoder.predict(data)\n",
        "    resized = np.resize(preds,(data.shape[0],preds.shape[1]*preds.shape[2]*preds.shape[3]))\n",
        "    return resized\n",
        "  \n",
        "  \n",
        "features = get_features(tampered)\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8sk-Qp7KpJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "fec4a132-9f97-410e-b740-f5fe529f6066"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn import svm\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "\n",
        "# Apply standard scaler to output from resnet50\n",
        "ss = StandardScaler()\n",
        "ss.fit(features)\n",
        "features = ss.transform(features)                               #\n",
        "\n",
        "# Take PCA to reduce feature space dimensionality\n",
        "pca = PCA(n_components=20, whiten=True)\n",
        "pca = pca.fit(features)\n",
        "print('Explained variance percentage = %0.2f' % sum(pca.explained_variance_ratio_))\n",
        "features = pca.transform(features)\n",
        "\n",
        "# Train classifier and obtain predictions for OC-SVM\n",
        "oc_svm_clf = svm.OneClassSVM(gamma=0.001, kernel='rbf', nu=0.08)  # Obtained using grid search\n",
        "if_clf = IsolationForest(contamination=0.08, max_features=1.0, max_samples=1.0, n_estimators=40)  # Obtained using grid search\n",
        "\n",
        "### FITTING ON THE METHOD ###\n",
        "\"\"\"\n",
        "oc_svm_clf.fit(features)\n",
        "if_clf.fit(features)\n",
        "\n",
        "#### Saving the Parameters to be used later ####\n",
        "filename = '/content/drive/My Drive/weights/AnoGANs/ocsvm-512.sav'     \n",
        "pickle.dump(oc_svm_clf,open(filename,'wb')) \n",
        "\n",
        "filename = '/content/drive/My Drive/weights/AnoGANs/if-512.sav'\n",
        "pickle.dump(if_clf,open(filename,'wb'))\n",
        "\"\"\"\n",
        "# Further compute accuracy, precision and recall for the two predictions sets obtained\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variance percentage = 0.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\noc_svm_clf.fit(features)\\nif_clf.fit(features)\\n\\n#### Saving the Parameters to be used later ####\\nfilename = '/content/drive/My Drive/weights/AnoGANs/ocsvm-512.sav'     \\npickle.dump(oc_svm_clf,open(filename,'wb')) \\n\\nfilename = '/content/drive/My Drive/weights/AnoGANs/if-512.sav'\\npickle.dump(if_clf,open(filename,'wb'))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5L34VFA3joJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### PREDICTION ON TRAINING SET ########\n",
        "filename1 = '/content/drive/My Drive/weights/AnoGANs/ocsvm-20.sav'  \n",
        "filename2 = '/content/drive/My Drive/weights/AnoGANs/if-512.sav'\n",
        "oc_svm_clf = pickle.load(open(filename1,'rb'))\n",
        "#if_clf = pickle.load(open(filename2,'rb'))\n",
        "\n",
        "def prediction(features):\n",
        "\n",
        "  #oc_svm_preds = oc_svm_clf.predict(X_test)                     #############################\n",
        "  #if_preds = if_clf.predict(X_test)                             #############################\n",
        "  oc_svm_preds = oc_svm_clf.predict(features)\n",
        "  #print(\"Prediction due to One Class SVM are : \")\n",
        "  #print(oc_svm_preds)\n",
        "  #print(\"          \")\n",
        "  #if_preds = if_clf.predict(features)\n",
        "  #print(\"Prediction due to Isolation Forest are : \")\n",
        "  #print(if_preds)\n",
        "\n",
        "  return oc_svm_preds,if_preds\n",
        "\n",
        "oc_svm_preds,if_preds = prediction(features)  "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrqVo2HGmjiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "243846ab-27b6-470f-8049-10ead43eec5d"
      },
      "source": [
        "count = 0\n",
        "cnt = 0\n",
        "for i in oc_svm_preds:\n",
        "  if i == -1:\n",
        "    count+=1\n",
        "\n",
        "for j in  if_preds:\n",
        "  if j == -1:\n",
        "    cnt +=1\n",
        "\n",
        "print(\"The correctly predicted for OCSVM are: \", count,\"/\",len(oc_svm_preds))   \n",
        "print(\"The correctly predicted for IF are: \", cnt,\"/\",len(if_preds))     "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The correctly predicted for OCSVM are:  2788 / 7491\n",
            "The correctly predicted for IF are:  556 / 7491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v1gwKhZ6d4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "ONE CLASS LEARNING WITH GMM MODELS \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjtvsSU98Y1",
        "colab_type": "text"
      },
      "source": [
        "# RECONSTRUCTING THE IMAGE BINARY MASK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tF1iL_S-DvV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a08c2e2c-f6ff-4a0f-89e0-06a8374fb1a7"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "WE HAVE A LIST WITH THE LABELS AND THE CORRESPONDING TOP-LEFT COORDINATE OF THE IMAGE PATCH\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWE HAVE A LIST WITH THE LABELS AND THE CORRESPONDING TOP-LEFT COORDINATE OF THE IMAGE PATCH\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGigpugS-8QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "image = cv2.imread('/content/tampered_casia2.jpg',1)\n",
        "stepsize = 64\n",
        "(winW,winH) = (64,64)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG8Ybbyn7DZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##DEFINING THE SLIDING WINDOW\n",
        "def sliding_window(image, stepSize, windowSize):\n",
        "\t# slide a window across the image\n",
        "\tfor y in range(0, image.shape[0], stepSize):\n",
        "\t\tfor x in range(0, image.shape[1], stepSize):\n",
        "\t\t\t# yield the current window\n",
        "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjQODkk8BVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ARRAY IS GENERATED WITH ROWS CORRESPONDING TO IMAGE OF THE EACH BLOCK\n",
        "arr_flat = []\n",
        "arr = []\n",
        "coord_arr = []\n",
        "def arr_generate(image):\n",
        "\n",
        "  for (x, y, window) in sliding_window(image, stepSize=stepsize, windowSize=(winW, winH)):\n",
        "    if window.shape[0]!=winH or window.shape[1]!= winW:\n",
        "      continue \n",
        "    # if count == 1:\n",
        "    #   arr_flat = []\n",
        "    #   count +=1\n",
        "    coord_arr.append((x,y)) #### storing the top-left coordinates of each block\n",
        "    window = window.tolist()\n",
        "    arr_flat.append(window)  \n",
        "\n",
        "  return arr_flat\n",
        "\n",
        "arr = arr_generate(image)\n",
        "print(len(arr))\n",
        "print(len(coord_arr))  "
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n74qxQpMDzpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b0908f4-7733-45aa-a1cc-69008862e4fb"
      },
      "source": [
        "arr1 = np.array(arr)\n",
        "arr1.shape"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 64, 64, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_sZyztTt65s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### LOADING THE TRAINED WEIGHTS ####\n",
        "import os \n",
        "if os.path.isfile(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\"):\n",
        "  autoencoder.load_weights(\"/content/drive/My Drive/weights/AnoGANs/autoencoder.h5\")\n",
        "\n",
        "\n",
        "##### Extracting the Features #####  \n",
        "def get_features(data):\n",
        "    preds = encoder.predict(data)\n",
        "    resized = np.resize(preds,(data.shape[0],preds.shape[1]*preds.shape[2]*preds.shape[3]))\n",
        "  return resized\n",
        "  \n",
        "features = get_features(arr1)\n",
        "len(features)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_QlcMhprqsw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6103f65e-c7bb-46d6-c40f-5b39f23b4a14"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn import svm\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "def transform(features,n_components = 20):\n",
        "\n",
        "  # Apply standard scaler to output from resnet50\n",
        "  ss = StandardScaler()\n",
        "  ss.fit(features)\n",
        "  features = ss.transform(features)\n",
        "\n",
        "  # Take PCA to reduce feature space dimensionality\n",
        "  pca = PCA(n_components=n_components, whiten=True)\n",
        "  pca = pca.fit(features)\n",
        "  print('Explained variance percentage = %0.2f' % sum(pca.explained_variance_ratio_))\n",
        "  features = pca.transform(features)\n",
        "\n",
        "  return features\n",
        "\n",
        "transformed_features = transform(features,20)\n"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variance percentage = 0.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk1lzpOGtlyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(feat):\n",
        "  # Train classifier and obtain predictions for OC-SVM\n",
        "  oc_svm_clf = svm.OneClassSVM(gamma=0.001, kernel='rbf', nu=0.08)  # Obtained using grid search\n",
        "  if_clf = IsolationForest(contamination=0.08, max_features=1.0, max_samples=1.0, n_estimators=40)  # Obtained using grid search\n",
        "  filename1 = '/content/drive/My Drive/weights/AnoGANs/ocsvm-512.sav'\n",
        "  filename2 = '/content/drive/My Drive/weights/AnoGANs/if-512.sav'\n",
        "  oc_svm_clf = pickle.load(open(filename1,'rb'))\n",
        "  if_clf = pickle.load(open(filename2,'rb'))\n",
        "  oc_svm_preds = oc_svm_clf.predict(feat)\n",
        "  if_preds = if_clf.predict(feat)\n",
        "  \n",
        "  return oc_svm_preds,if_preds\n",
        "\n",
        "oc_svm_preds,if_preds = predict(transformed_features)  "
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_AyTcTJunte",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ed33575-3795-4a2d-dfc2-9aadb2bd66ad"
      },
      "source": [
        "print(\"The labels for OCSVM are : \")\n",
        "print(oc_svm_preds)\n",
        "print(\" \")\n",
        "print(\"The labels for If are : \")\n",
        "print(if_preds)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  1  1 -1 -1 -1  1  1  1  1 -1  1 -1 -1 -1  1  1 -1 -1  1  1 -1 -1  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or_92-_Cu69L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tampered_ids = [ i for i,val in enumerate(oc_svm_preds) if val == -1]\n",
        "tampered_coord = []\n",
        "for i in tampered_ids:\n",
        "  tampered_coord.append(coord_arr[i])\n",
        "\n",
        "print(len(tampered_coord))"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jePAR3ZWwwnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "8c305c9d-83ce-43d5-d578-b09eedfb1b99"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "### creating an array of the same size as that of resized input image\n",
        "duplication = np.zeros((image.shape[0],image.shape[1]))\n",
        "\n",
        "### Giving values to the duplicated regions as 255\n",
        "for coord in tampered_coord:\n",
        "  duplication[coord[0]:coord[0]+winW,coord[1]:coord[1]+winH] = 255 \n",
        "\n",
        "cv2_imshow(duplication)  "
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAAAAACfHP67AAACVElEQVR4nO3c0QmAMAxAQSvuv3LdwQiH+G6AJvDIb9eBbTx/4fknnv97BcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArADYpReY0v//T3UBWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArALb29IFX1nju6/t3AVgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAFYArABYAbACYAXACoAVACsAVgCsAFgBsAJgBcAKgBUAKwBWAKwAWAGwAmAFwAqAFQArAHYD9soFAjhJSNYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=384x256 at 0x7F30F691E4A8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w-ZGJ9v1Hfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}